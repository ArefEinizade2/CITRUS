{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb05b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/benaziza-22/anaconda3/envs/ima/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.regression import MeanAbsolutePercentageError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800c2828",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "085411da",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 6\n",
    "M_hat = M\n",
    "n_epochs = 300\n",
    "val_len = 0.1\n",
    "test_len = 0.1\n",
    "lr = 1e-2\n",
    "batch_size = 1024\n",
    "enable_progress_bar = True\n",
    "horizons = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ccf24d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tsl.nn.models import DCRNNModel, GraphWaveNetModel, GatedGraphNetworkModel, RNNEncGCNDecModel\n",
    "from tsl.nn.models import STCNModel, EvolveGCNModel, GRUGCNModel, AGCRNModel, GRINModel\n",
    "import torch.nn as nn\n",
    "\n",
    "from tsl.nn.blocks.encoders import RNN\n",
    "from tsl.nn.layers import NodeEmbedding, DiffConv\n",
    "from einops.layers.torch import Rearrange  # reshape data with Einstein notation\n",
    "\n",
    "\n",
    "lr_milestones = [ 40, 80, 120]\n",
    "scheduler_kwargs = dict(milestones=lr_milestones, gamma=0.25)\n",
    "scheduler_class = MultiStepLR\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tsl.metrics.torch import MaskedMSE, MaskedMAE, MaskedMAPE\n",
    "from tsl.engines import Predictor\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "import shutil\n",
    "# Get the current script file path\n",
    "# script_path = os.path.realpath(__file__)\n",
    "# Specify the destination file path\n",
    "destination_path = './PemsBay_Results/'\n",
    "\n",
    "if not os.path.isdir(destination_path):  \n",
    "    os.mkdir(destination_path)\n",
    "# Copy the current script to the destination\n",
    "# shutil.copy2(script_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9c972cd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.0\n",
      "  PyG version: 2.6.1\n",
      "  tsl version: 0.9.5\n"
     ]
    }
   ],
   "source": [
    "from layers import CPGNN_ST, CPGNN_ST_v2, CPGNN_ST_v3, CITRUS, SGPModel\n",
    "import networkx as nx\n",
    "from Utilsss import get_evcs_evals\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, ProgressBar\n",
    "\n",
    "import tsl\n",
    "import torch\n",
    "import torch_geometric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"  PyG version: {torch_geometric.__version__}\")\n",
    "print(f\"  tsl version: {tsl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f7a7238",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Plotting functions ###############\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "np.set_printoptions(edgeitems=3, precision=3)\n",
    "torch.set_printoptions(edgeitems=2, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5133478",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Utility functions ################\n",
    "def print_matrix(matrix):\n",
    "    return pd.DataFrame(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e88b713",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def print_model_size(model):\n",
    "    tot = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "    out = f\"Number of model ({model.__class__.__name__}) parameters:{tot:10d}\"\n",
    "    print(\"=\" * len(out))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e17b6aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetrLA(length=34272, n_nodes=207, n_channels=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/benaziza-22/anaconda3/envs/ima/lib/python3.12/site-packages/tsl/datasets/metr_la.py:98: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(df.index[0], df.index[-1], freq='5T')\n",
      "/home/infres/benaziza-22/anaconda3/envs/ima/lib/python3.12/site-packages/tsl/datasets/metr_la.py:109: FutureWarning: The 'method' keyword in DataFrame.replace is deprecated and will be removed in a future version.\n",
      "  df = df.replace(to_replace=0., method='ffill')\n"
     ]
    }
   ],
   "source": [
    "from tsl.datasets import PemsBay\n",
    "from tsl.datasets import MetrLA\n",
    "\n",
    "# dataset = PemsBay(root='./PemsBay')\n",
    "dataset = MetrLA(root='./MetrLA')\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99472e9f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling period: <5 * Minutes>\n",
      "Has missing values: True\n",
      "Has exogenous variables: True\n",
      "Covariates: dist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>nodes</th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channels</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:00:00</th>\n",
       "      <td>64.38</td>\n",
       "      <td>67.62</td>\n",
       "      <td>67.12</td>\n",
       "      <td>61.50</td>\n",
       "      <td>66.88</td>\n",
       "      <td>68.75</td>\n",
       "      <td>65.12</td>\n",
       "      <td>67.12</td>\n",
       "      <td>59.62</td>\n",
       "      <td>62.75</td>\n",
       "      <td>...</td>\n",
       "      <td>45.62</td>\n",
       "      <td>65.50</td>\n",
       "      <td>64.50</td>\n",
       "      <td>66.43</td>\n",
       "      <td>66.88</td>\n",
       "      <td>59.38</td>\n",
       "      <td>69.00</td>\n",
       "      <td>59.25</td>\n",
       "      <td>69.00</td>\n",
       "      <td>61.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:05:00</th>\n",
       "      <td>62.67</td>\n",
       "      <td>68.56</td>\n",
       "      <td>65.44</td>\n",
       "      <td>62.44</td>\n",
       "      <td>64.44</td>\n",
       "      <td>68.11</td>\n",
       "      <td>65.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>57.44</td>\n",
       "      <td>63.33</td>\n",
       "      <td>...</td>\n",
       "      <td>50.67</td>\n",
       "      <td>69.88</td>\n",
       "      <td>66.67</td>\n",
       "      <td>58.56</td>\n",
       "      <td>62.00</td>\n",
       "      <td>61.11</td>\n",
       "      <td>64.44</td>\n",
       "      <td>55.89</td>\n",
       "      <td>68.44</td>\n",
       "      <td>62.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:10:00</th>\n",
       "      <td>64.00</td>\n",
       "      <td>63.75</td>\n",
       "      <td>60.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>66.50</td>\n",
       "      <td>66.25</td>\n",
       "      <td>64.50</td>\n",
       "      <td>64.25</td>\n",
       "      <td>63.88</td>\n",
       "      <td>65.38</td>\n",
       "      <td>...</td>\n",
       "      <td>44.12</td>\n",
       "      <td>69.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>59.25</td>\n",
       "      <td>68.12</td>\n",
       "      <td>62.50</td>\n",
       "      <td>65.62</td>\n",
       "      <td>61.38</td>\n",
       "      <td>69.86</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:15:00</th>\n",
       "      <td>64.00</td>\n",
       "      <td>63.75</td>\n",
       "      <td>60.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>66.50</td>\n",
       "      <td>66.25</td>\n",
       "      <td>64.50</td>\n",
       "      <td>64.25</td>\n",
       "      <td>63.88</td>\n",
       "      <td>65.38</td>\n",
       "      <td>...</td>\n",
       "      <td>44.12</td>\n",
       "      <td>69.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>59.25</td>\n",
       "      <td>68.12</td>\n",
       "      <td>62.50</td>\n",
       "      <td>65.62</td>\n",
       "      <td>61.38</td>\n",
       "      <td>69.86</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:20:00</th>\n",
       "      <td>64.00</td>\n",
       "      <td>63.75</td>\n",
       "      <td>60.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>66.50</td>\n",
       "      <td>66.25</td>\n",
       "      <td>64.50</td>\n",
       "      <td>64.25</td>\n",
       "      <td>63.88</td>\n",
       "      <td>65.38</td>\n",
       "      <td>...</td>\n",
       "      <td>44.12</td>\n",
       "      <td>69.00</td>\n",
       "      <td>56.50</td>\n",
       "      <td>59.25</td>\n",
       "      <td>68.12</td>\n",
       "      <td>62.50</td>\n",
       "      <td>65.62</td>\n",
       "      <td>61.38</td>\n",
       "      <td>69.86</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-06-27 23:35:00</th>\n",
       "      <td>65.00</td>\n",
       "      <td>65.89</td>\n",
       "      <td>68.56</td>\n",
       "      <td>61.67</td>\n",
       "      <td>32.83</td>\n",
       "      <td>54.56</td>\n",
       "      <td>62.44</td>\n",
       "      <td>63.33</td>\n",
       "      <td>59.22</td>\n",
       "      <td>65.33</td>\n",
       "      <td>...</td>\n",
       "      <td>52.89</td>\n",
       "      <td>69.00</td>\n",
       "      <td>65.11</td>\n",
       "      <td>55.67</td>\n",
       "      <td>66.33</td>\n",
       "      <td>62.44</td>\n",
       "      <td>66.78</td>\n",
       "      <td>64.89</td>\n",
       "      <td>69.67</td>\n",
       "      <td>62.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-06-27 23:40:00</th>\n",
       "      <td>61.38</td>\n",
       "      <td>65.62</td>\n",
       "      <td>66.50</td>\n",
       "      <td>62.75</td>\n",
       "      <td>32.83</td>\n",
       "      <td>50.50</td>\n",
       "      <td>62.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>65.25</td>\n",
       "      <td>67.12</td>\n",
       "      <td>...</td>\n",
       "      <td>54.00</td>\n",
       "      <td>69.25</td>\n",
       "      <td>60.12</td>\n",
       "      <td>60.50</td>\n",
       "      <td>67.25</td>\n",
       "      <td>59.38</td>\n",
       "      <td>66.00</td>\n",
       "      <td>61.25</td>\n",
       "      <td>69.00</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-06-27 23:45:00</th>\n",
       "      <td>67.00</td>\n",
       "      <td>59.67</td>\n",
       "      <td>69.56</td>\n",
       "      <td>61.00</td>\n",
       "      <td>32.83</td>\n",
       "      <td>44.78</td>\n",
       "      <td>64.22</td>\n",
       "      <td>63.78</td>\n",
       "      <td>59.78</td>\n",
       "      <td>57.67</td>\n",
       "      <td>...</td>\n",
       "      <td>51.33</td>\n",
       "      <td>67.89</td>\n",
       "      <td>64.33</td>\n",
       "      <td>57.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>62.67</td>\n",
       "      <td>68.67</td>\n",
       "      <td>63.33</td>\n",
       "      <td>67.44</td>\n",
       "      <td>61.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-06-27 23:50:00</th>\n",
       "      <td>66.75</td>\n",
       "      <td>62.25</td>\n",
       "      <td>66.00</td>\n",
       "      <td>59.62</td>\n",
       "      <td>32.83</td>\n",
       "      <td>53.00</td>\n",
       "      <td>64.29</td>\n",
       "      <td>64.12</td>\n",
       "      <td>60.88</td>\n",
       "      <td>66.25</td>\n",
       "      <td>...</td>\n",
       "      <td>51.12</td>\n",
       "      <td>69.38</td>\n",
       "      <td>61.62</td>\n",
       "      <td>60.50</td>\n",
       "      <td>65.62</td>\n",
       "      <td>66.38</td>\n",
       "      <td>69.50</td>\n",
       "      <td>63.00</td>\n",
       "      <td>67.88</td>\n",
       "      <td>63.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-06-27 23:55:00</th>\n",
       "      <td>65.11</td>\n",
       "      <td>66.89</td>\n",
       "      <td>66.78</td>\n",
       "      <td>61.22</td>\n",
       "      <td>32.83</td>\n",
       "      <td>49.56</td>\n",
       "      <td>65.78</td>\n",
       "      <td>65.11</td>\n",
       "      <td>63.00</td>\n",
       "      <td>61.67</td>\n",
       "      <td>...</td>\n",
       "      <td>56.00</td>\n",
       "      <td>67.44</td>\n",
       "      <td>64.89</td>\n",
       "      <td>60.89</td>\n",
       "      <td>64.22</td>\n",
       "      <td>66.44</td>\n",
       "      <td>68.44</td>\n",
       "      <td>63.56</td>\n",
       "      <td>68.67</td>\n",
       "      <td>61.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34272 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "nodes               773869 767541 767542 717447 717446 717445 773062 767620  \\\n",
       "channels                 0      0      0      0      0      0      0      0   \n",
       "2012-03-01 00:00:00  64.38  67.62  67.12  61.50  66.88  68.75  65.12  67.12   \n",
       "2012-03-01 00:05:00  62.67  68.56  65.44  62.44  64.44  68.11  65.00  65.00   \n",
       "2012-03-01 00:10:00  64.00  63.75  60.00  59.00  66.50  66.25  64.50  64.25   \n",
       "2012-03-01 00:15:00  64.00  63.75  60.00  59.00  66.50  66.25  64.50  64.25   \n",
       "2012-03-01 00:20:00  64.00  63.75  60.00  59.00  66.50  66.25  64.50  64.25   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2012-06-27 23:35:00  65.00  65.89  68.56  61.67  32.83  54.56  62.44  63.33   \n",
       "2012-06-27 23:40:00  61.38  65.62  66.50  62.75  32.83  50.50  62.00  67.00   \n",
       "2012-06-27 23:45:00  67.00  59.67  69.56  61.00  32.83  44.78  64.22  63.78   \n",
       "2012-06-27 23:50:00  66.75  62.25  66.00  59.62  32.83  53.00  64.29  64.12   \n",
       "2012-06-27 23:55:00  65.11  66.89  66.78  61.22  32.83  49.56  65.78  65.11   \n",
       "\n",
       "nodes               737529 717816  ... 772167 769372 774204 769806 717590  \\\n",
       "channels                 0      0  ...      0      0      0      0      0   \n",
       "2012-03-01 00:00:00  59.62  62.75  ...  45.62  65.50  64.50  66.43  66.88   \n",
       "2012-03-01 00:05:00  57.44  63.33  ...  50.67  69.88  66.67  58.56  62.00   \n",
       "2012-03-01 00:10:00  63.88  65.38  ...  44.12  69.00  56.50  59.25  68.12   \n",
       "2012-03-01 00:15:00  63.88  65.38  ...  44.12  69.00  56.50  59.25  68.12   \n",
       "2012-03-01 00:20:00  63.88  65.38  ...  44.12  69.00  56.50  59.25  68.12   \n",
       "...                    ...    ...  ...    ...    ...    ...    ...    ...   \n",
       "2012-06-27 23:35:00  59.22  65.33  ...  52.89  69.00  65.11  55.67  66.33   \n",
       "2012-06-27 23:40:00  65.25  67.12  ...  54.00  69.25  60.12  60.50  67.25   \n",
       "2012-06-27 23:45:00  59.78  57.67  ...  51.33  67.89  64.33  57.00  66.00   \n",
       "2012-06-27 23:50:00  60.88  66.25  ...  51.12  69.38  61.62  60.50  65.62   \n",
       "2012-06-27 23:55:00  63.00  61.67  ...  56.00  67.44  64.89  60.89  64.22   \n",
       "\n",
       "nodes               717592 717595 772168 718141 769373  \n",
       "channels                 0      0      0      0      0  \n",
       "2012-03-01 00:00:00  59.38  69.00  59.25  69.00  61.88  \n",
       "2012-03-01 00:05:00  61.11  64.44  55.89  68.44  62.88  \n",
       "2012-03-01 00:10:00  62.50  65.62  61.38  69.86  62.00  \n",
       "2012-03-01 00:15:00  62.50  65.62  61.38  69.86  62.00  \n",
       "2012-03-01 00:20:00  62.50  65.62  61.38  69.86  62.00  \n",
       "...                    ...    ...    ...    ...    ...  \n",
       "2012-06-27 23:35:00  62.44  66.78  64.89  69.67  62.33  \n",
       "2012-06-27 23:40:00  59.38  66.00  61.25  69.00  62.00  \n",
       "2012-06-27 23:45:00  62.67  68.67  63.33  67.44  61.22  \n",
       "2012-06-27 23:50:00  66.38  69.50  63.00  67.88  63.50  \n",
       "2012-06-27 23:55:00  66.44  68.44  63.56  68.67  61.78  \n",
       "\n",
       "[34272 rows x 207 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f\"Sampling period: {dataset.freq}\")\n",
    "print(f\"Has missing values: {dataset.has_mask}\")\n",
    "# print(f\"Percentage of missing values: {(1 - dataset.mask.mean()) * 100:.2f}%\")\n",
    "print(f\"Has exogenous variables: {dataset.has_covariates}\")\n",
    "print(f\"Covariates: {', '.join(dataset.covariates.keys())}\")\n",
    "\n",
    "\n",
    "print_matrix(dataset.dist)\n",
    "\n",
    "dataset.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fd01615",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default similarity: distance\n",
      "Available similarity options: {'distance'}\n",
      "==========================================\n",
      "Similarity matrix W:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    ...  197  198  199  \\\n",
       "0   1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  ... 0.00 0.00 0.12   \n",
       "1   0.00 1.00 0.39 0.00 0.00 0.00 0.00 0.39 0.00 0.00  ... 0.00 0.00 0.00   \n",
       "2   0.00 0.72 1.00 0.00 0.00 0.00 0.00 0.09 0.00 0.00  ... 0.00 0.00 0.00   \n",
       "3   0.00 0.00 0.00 1.00 0.63 0.00 0.01 0.00 0.00 0.00  ... 0.00 0.01 0.00   \n",
       "4   0.00 0.00 0.00 0.63 1.00 0.05 0.14 0.00 0.00 0.00  ... 0.00 0.00 0.00   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "202 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  ... 0.00 0.00 0.00   \n",
       "203 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  ... 0.00 0.00 0.00   \n",
       "204 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.22  ... 0.13 0.00 0.00   \n",
       "205 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  ... 0.00 0.00 0.00   \n",
       "206 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  ... 0.00 0.32 0.00   \n",
       "\n",
       "     200  201  202  203  204  205  206  \n",
       "0   0.00 0.00 0.00 0.00 0.00 0.00 0.00  \n",
       "1   0.00 0.03 0.00 0.00 0.00 0.00 0.00  \n",
       "2   0.00 0.00 0.00 0.00 0.00 0.00 0.00  \n",
       "3   0.00 0.00 0.00 0.00 0.00 0.00 0.00  \n",
       "4   0.00 0.00 0.00 0.00 0.00 0.00 0.00  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  \n",
       "202 0.00 0.00 1.00 0.08 0.00 0.00 0.00  \n",
       "203 0.00 0.00 0.00 1.00 0.00 0.00 0.00  \n",
       "204 0.00 0.00 0.00 0.00 1.00 0.00 0.00  \n",
       "205 0.00 0.00 0.00 0.00 0.00 1.00 0.00  \n",
       "206 0.00 0.00 0.00 0.00 0.00 0.00 1.00  \n",
       "\n",
       "[207 rows x 207 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Default similarity: {dataset.similarity_score}\")\n",
    "print(f\"Available similarity options: {dataset.similarity_options}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# sim = dataset.get_similarity(\"stcn\")  # or dataset.compute_similarity()\n",
    "sim = dataset.get_similarity()  # or dataset.compute_similarity()\n",
    "print(\"Similarity matrix W:\")\n",
    "print_matrix(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18b60d88",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index (2, 2626):\n",
      " [[  0   0   0 ... 206 206 206]\n",
      " [ 13  36  37 ... 163 187 198]]\n",
      "edge_weight (2626,):\n",
      " [0.261 0.519 0.509 ... 0.621 0.278 0.649]\n"
     ]
    }
   ],
   "source": [
    "connectivity = dataset.get_connectivity(threshold=0.1,\n",
    "                                        include_self=False,\n",
    "                                        layout=\"edge_index\",\n",
    "                                        force_symmetric=True)\n",
    "\n",
    "edge_index, edge_weight = connectivity\n",
    "\n",
    "print(f'edge_index {edge_index.shape}:\\n', edge_index)\n",
    "print(f'edge_weight {edge_weight.shape}:\\n', edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33b37531",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A (207, 207):\n",
      "Sparse edge weights:\n",
      " [0.261 0.519 0.509 ... 0.621 0.278 0.649]\n"
     ]
    }
   ],
   "source": [
    "from tsl.ops.connectivity import edge_index_to_adj\n",
    "\n",
    "adj = edge_index_to_adj(edge_index, edge_weight)\n",
    "print(f'A {adj.shape}:')\n",
    "print_matrix(adj)\n",
    "\n",
    "print(f'Sparse edge weights:\\n', adj[edge_index[1], edge_index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e848d15c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatioTemporalDataset(n_samples=34264, n_nodes=207, n_channels=1)\n"
     ]
    }
   ],
   "source": [
    "from tsl.data import SpatioTemporalDataset\n",
    "\n",
    "torch_dataset = SpatioTemporalDataset(target=dataset.dataframe(),\n",
    "                                      connectivity=connectivity,\n",
    "                                      mask=dataset.mask,\n",
    "                                      horizon=horizons,\n",
    "                                      window=M_hat,\n",
    "                                      stride=1)\n",
    "print(torch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09af197c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(\n",
      "  input=(x=[t=6, n=207, f=1], edge_index=[2, e=2626], edge_weight=[e=2626]),\n",
      "  target=(y=[t=3, n=207, f=1]),\n",
      "  has_mask=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = torch_dataset[0]\n",
    "# torch_dataset2 = torch_dataset[:1000]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f13f8aa5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "a = sample.input.to_dict()\n",
    "\n",
    "b = sample.target.to_dict()\n",
    "\n",
    "# if sample.has_mask:\n",
    "#     print(sample.mask)\n",
    "# else:\n",
    "#     print(\"Sample has no mask.\")\n",
    "\n",
    "# if sample.has_transform:\n",
    "#     print(sample.transform)\n",
    "# else:\n",
    "#     print(\"Sample has no transformation functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8dd7d0d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 't n f', 'mask': 't n f', 'edge_index': '2 e', 'edge_weight': 'e', 'y': 't n f'}\n",
      "==================   Or we can print patterns and shapes together   ==================\n",
      "Data(\n",
      "  input=(x=[t=6, n=207, f=1], edge_index=[2, e=2626], edge_weight=[e=2626]),\n",
      "  target=(y=[t=3, n=207, f=1]),\n",
      "  has_mask=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sample.pattern)\n",
    "print(\"==================   Or we can print patterns and shapes together   ==================\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b74c24fb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticBatch(\n",
      "  input=(x=[b=5, t=6, n=207, f=1], edge_index=[2, e=2626], edge_weight=[e=2626]),\n",
      "  target=(y=[b=5, t=3, n=207, f=1]),\n",
      "  has_mask=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = torch_dataset[:5]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aebdaee4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "from tsl.data.datamodule import (SpatioTemporalDataModule,\n",
    "                                 TemporalSplitter)\n",
    "from tsl.data.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize data using mean and std computed over time and node dimensions\n",
    "scalers = {'target': StandardScaler(axis=(0, 1))}\n",
    "\n",
    "# Split data sequentially:\n",
    "#   |------------ dataset -----------|\n",
    "#   |--- train ---|- val -|-- test --|\n",
    "splitter = TemporalSplitter(val_len=val_len, test_len=test_len)\n",
    "\n",
    "dm = SpatioTemporalDataModule(\n",
    "    dataset=torch_dataset,\n",
    "    scalers=scalers,\n",
    "    splitter=splitter,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# print(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24e0ab52",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Train dataloader: size=27749}\n",
      "{Validation dataloader: size=3077}\n",
      "{Test dataloader: size=3426}\n",
      "{Predict dataloader: None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dm.setup()\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "538faf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 16    #@param\n",
    "hidden_size = 32   #@param\n",
    "rnn_layers = 1     #@param\n",
    "gnn_kernel = 2   #@param\n",
    "\n",
    "input_size = torch_dataset.n_channels   # 1 channel\n",
    "n_nodes = torch_dataset.n_nodes         # 207 nodes\n",
    "horizon = torch_dataset.horizon         # 12 time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f6c3404",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "CGP-GNN:"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[323, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/benaziza-22/m2ds/graph_ml/CITRUS-Graph_ML/MetrLA_PemsBay/Utilsss.py:31: RuntimeWarning: k >= N - 1 for N * N square matrix. Attempting to use scipy.linalg.eig instead.\n",
      "  evals, evecs = sparse.linalg.eigs(L_sparse, k=K_list[0], return_eigenvectors=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot use scipy.linalg.eig for sparse A with k >= N - 1. Use scipy.linalg.eig(A.toarray()) or reduce k.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# G = nx.path_graph(N[0])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# G = nx.from_numpy_matrix(np.array(adj))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# AA = (adj+adj.T)/2\u001b[39;00m\n\u001b[1;32m     14\u001b[0m Graph_List \u001b[38;5;241m=\u001b[39m [nx\u001b[38;5;241m.\u001b[39mfrom_numpy_array(np\u001b[38;5;241m.\u001b[39marray(adj)), nx\u001b[38;5;241m.\u001b[39mpath_graph(N[\u001b[38;5;241m1\u001b[39m])]\n\u001b[0;32m---> 16\u001b[0m evecs, evals, L_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_evcs_evals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGraph_List\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(evals)):\n\u001b[1;32m     19\u001b[0m     evals[ii] \u001b[38;5;241m=\u001b[39m evals[ii]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/m2ds/graph_ml/CITRUS-Graph_ML/MetrLA_PemsBay/Utilsss.py:31\u001b[0m, in \u001b[0;36mget_evcs_evals\u001b[0;34m(Graph_list, K_list)\u001b[0m\n\u001b[1;32m     27\u001b[0m L_normalized_sparse_list \u001b[38;5;241m=\u001b[39m [L_sparse]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(K_list)\n\u001b[0;32m---> 31\u001b[0m evals, evecs \u001b[38;5;241m=\u001b[39m \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_eigenvectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m evals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(evals\u001b[38;5;241m.\u001b[39mreal)\n\u001b[1;32m     33\u001b[0m evals \u001b[38;5;241m=\u001b[39m evals\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/anaconda3/envs/ima/lib/python3.12/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1280\u001b[0m, in \u001b[0;36meigs\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, OPpart)\u001b[0m\n\u001b[1;32m   1275\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk >= N - 1 for N * N square matrix. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1276\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to use scipy.linalg.eig instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1277\u001b[0m               \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(A):\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use scipy.linalg.eig for sparse A with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk >= N - 1. Use scipy.linalg.eig(A.toarray()) or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1282\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reduce k.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, LinearOperator):\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use scipy.linalg.eig for LinearOperator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1285\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA with k >= N - 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot use scipy.linalg.eig for sparse A with k >= N - 1. Use scipy.linalg.eig(A.toarray()) or reduce k."
     ]
    }
   ],
   "source": [
    "# CGP_GNN = CGP_GNN_STGNN_betterImp(A=torch.tensor(adj).to(device), M=M_hat)\n",
    "\n",
    "N = [n_nodes, M]\n",
    "K_list = list(np.array(N)-2)\n",
    "K_list = [323, M-2]\n",
    "\n",
    "\n",
    "# G = nx.path_graph(N[0])\n",
    "\n",
    "# G = nx.from_numpy_matrix(np.array(adj))\n",
    "\n",
    "# AA = (adj+adj.T)/2\n",
    "\n",
    "Graph_List = [nx.from_numpy_array(np.array(adj)), nx.path_graph(N[1])]\n",
    "\n",
    "evecs, evals, L_list = get_evcs_evals(Graph_List, K_list)\n",
    "\n",
    "for ii in range(len(evals)):\n",
    "    evals[ii] = evals[ii].to(device)\n",
    "    \n",
    "    \n",
    "CGP_GNN = CITRUS(input_size=input_size,\n",
    "                            n_nodes=n_nodes,\n",
    "                            horizon=horizon,\n",
    "                            emb_size=emb_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            rnn_layers=rnn_layers,\n",
    "                            gnn_kernel=gnn_kernel,\n",
    "                            edge_index=torch.tensor(edge_index).to(device),\n",
    "                            edge_weight=torch.tensor(edge_weight).to(device),\n",
    "                            mass = torch.ones(np.prod(N)).to(device),\n",
    "                            evals = evals,\n",
    "                            evecs = torch.tensor(evecs).to(device),\n",
    "                            C_width = 64,\n",
    "                            N_block = 3,\n",
    "                            single_t = True,\n",
    "                            use_gdc = [],\n",
    "                            num_nodes = N,\n",
    "                            last_activation=torch.nn.ReLU(), \n",
    "                            mlp_hidden_dims=[64, 64, 64, 64], \n",
    "                            dropout=False, \n",
    "                            with_MLP=True, \n",
    "                            diffusion_method='spectral', \n",
    "                            device = device,\n",
    "                            graph_wise=False)\n",
    "\n",
    "              \n",
    "print(CGP_GNN)\n",
    "print_model_size(CGP_GNN)\n",
    "\n",
    "loss_fn = MaskedMAE()\n",
    "\n",
    "metrics = {'mse': MaskedMSE(),\n",
    "           'mae': MaskedMAE(),\n",
    "           'mape': MaskedMAPE()}\n",
    "\n",
    "\n",
    "# setup predictor_CGP_GNN\n",
    "# setup predictor\n",
    "predictor_CGP_GNN = Predictor(\n",
    "    model=CGP_GNN,                   # our initialized model\n",
    "    optim_class=torch.optim.Adam,  # specify optimizer to be used...\n",
    "    optim_kwargs={'lr': lr},    # ...and parameters for its initialization\n",
    "    loss_fn=loss_fn,               # which loss function to be used\n",
    "    metrics=metrics,\n",
    "# metrics to be logged during train/val/test\n",
    ")\n",
    "logger_CGP_GNN = TensorBoardLogger(save_dir=\"FINAL_PemsBay_M6_H3\", name=\"FINAL_PemsBay_M6_H3\", version=0)\n",
    "\n",
    "checkpoint_callback_CGPGNN = ModelCheckpoint(\n",
    "    dirpath='FINAL_PemsBay_M6_H3',\n",
    "    save_top_k=1,\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer_CGP_GNN = pl.Trainer(max_epochs=n_epochs,\n",
    "                      logger=logger_CGP_GNN,\n",
    "                      accelerator='gpu',\n",
    "                      devices=[1], \n",
    "#                      limit_train_batches=train_batches,  # end an epoch after 100 updates\n",
    "                      callbacks=[checkpoint_callback_CGPGNN],\n",
    "                      enable_progress_bar=enable_progress_bar)\n",
    "\n",
    "t_CGPGNN = time.time()\n",
    "trainer_CGP_GNN.fit(predictor_CGP_GNN, datamodule=dm)\n",
    "elapsed = time.time() - t_CGPGNN\n",
    "print('>>>>>>>>>>>>>>>>>>>> CGP-GNN training time, Elapsed: %s' % round(elapsed/60,2), ' minutes')\n",
    "\n",
    "predictor_CGP_GNN.load_model(checkpoint_callback_CGPGNN.best_model_path)\n",
    "predictor_CGP_GNN.freeze()\n",
    "\n",
    "\n",
    "\n",
    "CGP_GNN_results = trainer_CGP_GNN.test(predictor_CGP_GNN, datamodule=dm);\n",
    "\n",
    "\n",
    "#% Detailed metrics:\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "XX = dm.testset\n",
    "x_test = XX[:].x\n",
    "y_test = XX[:].y\n",
    "edge_index_test = XX[:].edge_index\n",
    "edge_weight_test = XX[:].edge_weight\n",
    "\n",
    "#CGPGNN:\n",
    "print(10*'*')\n",
    "print('CGPGNN:')    \n",
    "y_pred = trainer_CGP_GNN.predict(predictor_CGP_GNN, dm.test_dataloader())\n",
    "a = [y_pred[i]['y_hat'] for i in range(len(y_pred))]\n",
    "y_pred = torch.cat(a, axis=0)\n",
    "loss = nn.L1Loss()\n",
    "MAE = loss(y_pred, y_test)\n",
    "print(MAE)\n",
    "MAPE = MeanAbsolutePercentageError()\n",
    "MAPE = MAPE(y_pred, y_test)\n",
    "print(MAPE)\n",
    "loss = nn.MSELoss()\n",
    "MSE = loss(y_pred, y_test)\n",
    "print(MSE)\n",
    "RelativeMAE = MAE/torch.abs(y_test).mean()\n",
    "print(RelativeMAE)\n",
    "RelativeMSE = MSE/((y_test**2).mean())\n",
    "print(RelativeMSE)\n",
    "Metrics_CGPGNN = [MAE.numpy(), MAPE.numpy(), MSE.numpy(),\n",
    "                  RelativeMAE.numpy(), RelativeMSE.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4692b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*'*')\n",
    "print('CGPGNN:')\n",
    "print(Metrics_CGPGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524da554",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "elapsed = time.time() - t\n",
    "print('Elapsed: %s' % round(elapsed/60,2), ' minutes')\n",
    "print(600*'*')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
