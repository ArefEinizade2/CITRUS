{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0133c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from Utils.layers import CITRUS\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3980f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e4feb",
   "metadata": {},
   "source": [
    "The next function gives the eigenvector of a Cartesian product graph $\\mathcal{G}_\\diamond=\\mathcal{G}_1\\oplus...\\oplus\\mathcal{G}_P$ and also the list of egienvalues of the factor graphs as $\\mathbf{V}_\\diamond=\\mathbf{V}_1\\otimes...\\otimes \\mathbf{V}_P$ and $\\lambda_{list}=[\\boldsymbol{\\lambda}_1,...,\\boldsymbol{\\lambda}_P]$, where the EVD form of the $p$-th factor Laplacian:\n",
    "$\\mathbf{L}_p=\\mathbf{V}_p diag(\\boldsymbol{\\lambda}_p)\\mathbf{V}^\\top_p$ for $p=1,...,P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66901c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_evec_evals(L_normalized_sparse_list, k_list):\n",
    "    evals, evecs = sparse.linalg.eigs(L_normalized_sparse_list[0], k=k_list[0], return_eigenvectors=True)\n",
    "    evals = torch.tensor(evals.real)\n",
    "    evals = evals.to(torch.float32)\n",
    "    evals_list = [evals]\n",
    "    evecs=torch.tensor(evecs.real).to(torch.float32)        \n",
    "    evecs_kron = evecs\n",
    "    \n",
    "    for p in range(1, len(L_normalized_sparse_list)):\n",
    "\n",
    "        evals, evecs = sparse.linalg.eigs(L_normalized_sparse_list[p], k=k_list[p], return_eigenvectors=True)\n",
    "        evals = torch.tensor(evals.real)\n",
    "        evals = evals.to(torch.float32)\n",
    "        evals_list.append(evals)\n",
    "        evecs = torch.tensor(evecs.real)        \n",
    "        evecs_kron = torch.kron(evecs_kron, evecs).to(torch.float32)\n",
    "    \n",
    "    return evals_list, evecs_kron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399c409",
   "metadata": {},
   "source": [
    "The next function gives a connected random Erdős-Rényi (ER) graph with the number of nodes $n$ and edge-probability of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85c5be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_connected_ER(n, p):\n",
    "    connected = False\n",
    "    while not connected:\n",
    "        G = nx.erdos_renyi_graph(n, p)\n",
    "        connected = nx.is_connected(G)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5cf3b",
   "metadata": {},
   "source": [
    "The nest function generates the factor ER graphs with the number of nodes $N_{list}$ and edge prpbblities of $p_{list}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "778f482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_factor_graphs(N_list, p_list):\n",
    "    \n",
    "    P = len(N_list)\n",
    "    \n",
    "    Adj_list = []\n",
    "    \n",
    "    G = gen_connected_ER(N_list[0], p_list[0])\n",
    "    L = nx.laplacian_matrix(G).toarray()\n",
    "    \n",
    "    # Compute degree matrix\n",
    "    A = nx.to_numpy_array(G)  \n",
    "    Adj_list.append(A)\n",
    "    degrees = np.sum(A, axis=1)\n",
    "    # D = np.diag(degrees)\n",
    "    \n",
    "    # Compute normalized Laplacian\n",
    "    D_sqrt_inv = np.diag(1.0 / np.sqrt(degrees))\n",
    "    L_normalized = np.dot(np.dot(D_sqrt_inv, L), D_sqrt_inv)\n",
    "    L_normalized = L_normalized/P\n",
    "    L_sparse = sparse.coo_matrix(L_normalized)\n",
    "    L_normalized_sparse_list = [L_sparse]\n",
    "    \n",
    "    \n",
    "    for p in range(1, P):\n",
    "        G = gen_connected_ER(N_list[p], p_list[p])\n",
    "        L = nx.laplacian_matrix(G).toarray()\n",
    "\n",
    "        # Compute degree matrix\n",
    "        A = nx.to_numpy_array(G)\n",
    "        Adj_list.append(A)\n",
    "        degrees = np.sum(A, axis=1)\n",
    "        # D = np.diag(degrees)\n",
    "\n",
    "        # Compute normalized Laplacian\n",
    "        D_sqrt_inv = np.diag(1.0 / np.sqrt(degrees))\n",
    "        L_normalized = np.dot(np.dot(D_sqrt_inv, L), D_sqrt_inv)\n",
    "        L_normalized = L_normalized/P\n",
    "        L_sparse = sparse.coo_matrix(L_normalized)\n",
    "        L_normalized_sparse_list.append(L_sparse)\n",
    "\n",
    "        \n",
    "    return L_normalized_sparse_list\n",
    "#%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed6dd1",
   "metadata": {},
   "source": [
    "The next function implements a way for performing $i$-th mode matricization operation on a given tensor $\\underline{\\mathbf{X}}\\in\\mathbb{R}^{N_1\\times...\\times N_i\\times...\\times N_P}$ as $\\underline{\\mathbf{X}}_{(i)}$, where $\\underline{\\mathbf{X}}_{(i)}\\in\\mathbb{R}^{N_i\\times\\tilde{N}_i}$ with $\\tilde{N}_i=\\prod_{p=1,p\\ne i}^{P}{N_p}$. For more details, please refer to the tensor decompostion nad rocessing literature, like [1] and [2]. \n",
    "\n",
    "[1]  T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,” SIAM Review, vol. 51,\n",
    "no. 3, pp. 455–500, 2009.\n",
    "\n",
    "[2] N. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E. Papalexakis, and C. Faloutsos,\n",
    "“Tensor decomposition for signal processing and machine learning,” IEEE Transactions on\n",
    "Signal Processing, vol. 65, no. 13, pp. 3551–3582, 2017.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c5b881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_n_matricization(tensor, mode):\n",
    "    \"\"\"\n",
    "    Perform mode-n matricization of a tensor.\n",
    "\n",
    "    Parameters:\n",
    "    tensor (ndarray): The input tensor of shape (I1, I2, ..., IN).\n",
    "    mode (int): The mode along which to matricize (0-based index).\n",
    "\n",
    "    Returns:\n",
    "    ndarray: The mode-n matricized tensor.\n",
    "    \"\"\"\n",
    "    # Move the mode-th axis to the first dimension\n",
    "    tensor = np.moveaxis(tensor, mode, 0)\n",
    "    # Get new shape: (size of mode-n, product of remaining dimensions)\n",
    "    new_shape = (tensor.shape[0], -1)\n",
    "    # Reshape tensor to new shape\n",
    "    matricized_tensor = tensor.reshape(new_shape)\n",
    "    return matricized_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d72a8",
   "metadata": {},
   "source": [
    "The next function rearranges a given mode-$i$ matricized form of a tensor ($\\underline{\\mathbf{X}}_{(i)}$) to produce the initil tensor $\\underline{\\mathbf{X}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7049f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_mode_n_matricization(matricized_tensor, original_shape, mode):\n",
    "    \"\"\"\n",
    "    Reverse mode-n matricization to reconstruct the original tensor.\n",
    "\n",
    "    Parameters:\n",
    "    matricized_tensor (ndarray): The matricized tensor of shape (I_n, prod(I_1, ..., I_{n-1}, I_{n+1}, ..., I_N)).\n",
    "    original_shape (tuple): The original shape of the tensor (I1, I2, ..., IN).\n",
    "    mode (int): The mode along which the matricization was performed (0-based index).\n",
    "\n",
    "    Returns:\n",
    "    ndarray: The reconstructed tensor with the original shape.\n",
    "    \"\"\"\n",
    "    # Determine the shape after expanding back to the original tensor's dimensions\n",
    "    new_shape = (original_shape[mode],) + tuple(dim for i, dim in enumerate(original_shape) if i != mode)\n",
    "    \n",
    "    # Reshape the matricized tensor back to this expanded shape\n",
    "    reshaped_tensor = matricized_tensor.reshape(new_shape)\n",
    "    \n",
    "    # Reverse the axis reordering to get back to the original shape\n",
    "    reconstructed_tensor = np.moveaxis(reshaped_tensor, 0, mode)\n",
    "    return reconstructed_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2361fad",
   "metadata": {},
   "source": [
    "Next, using some parameters, the the facor graphs, and their eigenvalue-eigenvector forms are generated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "280c5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Generate the graphs:\n",
    "p_list = [0.3, 0.3, 0.05]\n",
    "N_list = [10, 15, 20]\n",
    "L_list = gen_factor_graphs(N_list, p_list)\n",
    "k = np.array(N_list)-2\n",
    "k_list = [N_list[0] - 2, N_list[1] - 2, N_list[2] - 2]\n",
    "evals, evecs = get_selected_evec_evals(L_list, k_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5884ce",
   "metadata": {},
   "source": [
    "Then, the parameters for generating the data tensor $\\underline{\\mathbf{X}}$ and building the CITRUS blocks are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fea = [4, 2]\n",
    "N_block = 32\n",
    "C_width = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdc116",
   "metadata": {},
   "source": [
    "In the following, a sample data is generated first and then a CTRUS model with $N_{blocks}=32$ blocks is built to transform the (matricized) given data $\\underline{\\mathbf{X}}^\\top_{(4)}\\in\\mathbb{R}^{[\\overbrace{10\\times15\\times20}^{3000}]\\times4}$ to the (matricized) output data $\\underline{\\mathbf{Y}}^\\top_{(4)}\\in\\mathbb{R}^{[\\overbrace{10\\times15\\times20}^{3000}]\\times2}$, where $\\underline{\\mathbf{X}}\\in\\mathbb{R}^{10\\times15\\times20\\times4}$ and $\\underline{\\mathbf{Y}}\\in\\mathbb{R}^{10\\times15\\times20\\times2}$. For more details, please refer to our paper [3].\n",
    "\n",
    "[3] Einizade, Aref, Fragkiskos D. Malliaros, and Jhony H. Giraldo. \"Continuous Product Graph Neural Networks.\" arXiv preprint arXiv:2405.18877 (2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a036c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tensor.shape:  torch.Size([10, 15, 20, 4])\n",
      "X_unfolded.shape:  torch.Size([3000, 4])\n",
      "out_unfolded.shape:  torch.Size([3000, 2])\n",
      "out_tensor.shape:  (10, 15, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.randn(N_list[0], N_list[1], N_list[2], Fea[0])\n",
    "\n",
    "print('X_tensor.shape: ', X_tensor.shape)\n",
    "\n",
    "X_unfolded = torch.tensor(mode_n_matricization(X_tensor.numpy(), 3).T).to(device)\n",
    "\n",
    "print('X_unfolded.shape: ', X_unfolded.shape)\n",
    "\n",
    "## Run mode\n",
    "            \n",
    "model = CITRUS(k = np.prod(k), C_in = Fea[0], C_out = Fea[-1], C_width = C_width, num_nodes = N_list,\n",
    "          N_block = N_block, single_t = False, use_gdc = [],\n",
    "            last_activation = lambda x : x,\n",
    "            diffusion_method = 'spectral',\n",
    "            with_MLP = True,\n",
    "            dropout = True,\n",
    "            device = device)\n",
    "            \n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model_CPGNN = model\n",
    "    \n",
    "parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "parameters_name= [ name for (name, param) in model.named_parameters() if param.requires_grad]\n",
    "\n",
    "# Move to device\n",
    "mass=torch.ones(np.prod(N_list)).to(device)\n",
    "\n",
    "for ii in range(len(evals)):\n",
    "    evals[ii] = evals[ii].to(device)\n",
    "evecs=evecs.to(device)\n",
    "\n",
    "out_unfolded = model(0, X_unfolded, [], mass=mass, L=L_list, evals=evals, evecs=evecs) #GITHUUUUUUUUUB\n",
    "\n",
    "print('out_unfolded.shape: ', out_unfolded.shape)\n",
    "\n",
    "out_tensor = reverse_mode_n_matricization(out_unfolded.cpu().detach().numpy(), (N_list[0], N_list[1], N_list[2], Fea[-1]), 3)\n",
    "\n",
    "print('out_tensor.shape: ', out_tensor.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
